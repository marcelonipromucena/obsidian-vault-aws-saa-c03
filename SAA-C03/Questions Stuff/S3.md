#s3 #s3-bucket-url
- Valid URLs for bucket:
		- https://bucketteste.s3.us-east-1.amazonaws.com
			- https:// **(bucket)** . s3 . **(region)** . amazonaws.com
		- https://s3.us-east-1.amazonaws.com/bucketteste
			- https:// s3 . **(region)** . amazonaws.com/ **(bucket)**
## Scenario 01
#s3 #s3-charges #s3ta
- NASA scientist is trying to upload high-res image to S3;
- Image size is 3GB;
- He is using S3 Transfer Acceleration;
- S3TA did not result in an accelerated transfer;
- How AWS would charge this image transfer?
- Answer:
	- Cannot be any option that says "**AWS charges S3 transfer for the image upload.**"
	- <mark style="background: #BBFABBA6;">AWS won't charge for the image upload, because with S3TA you pay ONLY for transfers that ARE INDEED accelerated.</mark>

## Scenario 02
#s3 #s3-security #s3-encryption #s3-sse-kms
- Company is build an application;
- It will deal with **sensitive health information**;
- Backups of the user data **must be kept encrypted in S3**;
- Company does not want to **provide its own encryption keys**;
- **But** still wants to **maintain an audit trail of when an encryption key _was used and by whom_**;
- Which is the best solution?
- Answer:
	- Can't be **client-side encryption** since company doesn't want to provide its own encryption keys;
	- Can't be **SSE-S3** since company wants to maintain an audit trail and with SSE-S3 each object is encrypted with an **_unique key_**;
	- Can't be **SSE-C** since you can't audit trail the usage of the encryption keys, and the company doesn't want to provide its own encryption keys;
	- <mark style="background: #BBFABBA6;">SSE-KMS is the right answer since it takes away the need for the company to provide its own keys AND you can maintain an audit trail of when the key was used and by whom.</mark>

## Scenario 03
- Company uses S3 to store sensitive customer data;
- Company has defined different retention periods for different objects based on compliance requirements;
- **Retention rules aren't working as expected**;
- Which one is a valid configuration for **setting up retention periods for objects in S3 buckets**;
- Answer:
	- The bucket's default settings will **NOT** override any explicit retention mode or period you request on an object version;
	- You actually **CAN** place a retention period on an object version either explicitly or through a bucket default setting;
	- You **CANNOT** specify a `Retain Until Date` for the object version. Instead you specify a `Duration`.
	- <mark style="background: #BBFABBA6;">Different versions of a single object can have different retention modes and periods;</mark>
	- <mark style="background: #BBFABBA6;">When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version.</mark>

## Scenario 04
#s3 #s3-limits #s3-scaling
- Company uses S3 as file-hosting service;
- Customer files are uploaded directly to S3 under a **single bucket**;
- Company started seeing scalability issues where customer file uploads have started failing during the peak access hours with more than **5000 requests per second**;
- **MOST resource-efficient** and **cost-optimal** way of addressing this issue?
- Answer:
	- Change the app's architecture to create a new S3 bucket for each day's data and then upload the daily files directly under that day's bucket **is inefficient and s3 buckets must be globally unique**; 
	- Change the app's architecture to create a new S3 bucket for each customer and then upload each customer's file directly under the respective bucket **is inefficient and s3 buckets must be globally unique**; 
	- Change the app's architecture to use EFS instead of S3: **EFS is costlier compared to S3**
	- <mark style="background: #BBFABBA6;">Change the app's architecture to create a Customer-Specific custom prefixes within a SINGLE BUCKET and then upload the daily files into those prefixes locations.</mark>
	- <mark style="background: #BBFABBA6;">S3 automatically scales to high request rates. For example: your application can achieve at least <mark style="background: #FFB86CA6;">3.500 PUT/COPY/POST/DELETE</mark> or <mark style="background: #FFB86CA6;">5.500 GET/HEAD</mark> requests <mark style="background: #FF5582A6;">PER SECOND PER PREFIX WITHIN A SINGLE BUCKET</mark>. </mark>
	- <mark style="background: #BBFABBA6;">For example, if you create<mark style="background: #7ce616;">10 prefixes</mark> in an S3 bucket to parallelize reads, you could scale your read performance to <mark style="background: #ef6c00;">55,000 READ REQUESTS PER SECOND!</mark></mark>

## Scenario 05
#s3 #s3-lifecycle 
- Company stores its assets on S3;
- Assets are accessed by a large number of users for the first few days;
- Then the frequency of **access falls down drastically after a week**;
- Although the assets would be accessed occasionally after the first week, they **must continue to be immediately accessible when required**;
- Company wants to reduce cost as much as possible.
- Can you suggest a way to lower storage cost while fulfilling the business requirements?
- Answer:
	- Can't be **Lifecycle policy -> S3 Standard IA** after 30 days. It fulfills **almost** all requirements but isn't the **cheapest one.**;
	- Can't be **Lifecycle Policy -> One Zone-IA** after 7 days because the minimum storage duration is **_30 days before you can transition objects from Standard to One Zone-IA or Standard-IA_**
	- Can't be **Lifecycle Policy -> Standard-IA** after 7 days because the minimum storage duration is **_30 days before you can transition objects from Standard to One Zone-IA or Standard-IA_**
	- <mark style="background: #BBFABBA6;">Lifecycle Policy -> One-Zone IA after 30 days is the right answer since it respects the minimum storage duration to transition objects (30 days) and is the cheapest option.</mark>

## Scenario 06
#s3 #s3-versioning 
- Which feature of S3 can only be suspended once they have been enabled?
- Answer:
	- <mark style="background: #BBFABBA6;">Versioning.</mark>
	- Once you version-enable a bucket, <mark style="background: #FF5582A6;">it can never return to an unversioned state.</mark>

## Scenario 07
- Company needs to enforce compliance and regulatory guidelines for objects stored in S3;
- They want to provide adequate **protection against accidental deletion of objects**;
- What do you recommend?
- Answer:
	- <mark class="hltr-green">Enable Versioning;</mark>
	- <mark class="hltr-green">Enable MFA Delete</mark>;

## Scenario 08
#s3 #s3-storage-classes
- Company delivers billions of hours of content from S3 to customers;
- S3 also serves as **Data Lake**;
- Data lake has a **staging zone** where intermediary query results are kept only for **24 hours**;
- Which storage-class has the most cost-effective strategy?
- Answer:
	- **S3 Glacier Instant Retrieval** has a minimum storage duration charge of **90 days**;
	- **S3 Standard IA** has a minimum storage duration charge of **30 days**;
	- **S3 One Zone-IA** has a minimum storage duration charge of **30 days**;
	- <mark class="hltr-green">S3 Standard has <mark class="hltr-red">NO MINIMUM STORAGE DURATION CHARGE</mark> and <mark class="hltr-red">NO RETRIEVAL FEE</mark>, hence the correct answer.</mark>


## Scenario 09
- Company generates and accesses the audit reports only **twice in a financial year**;
- Department uses **Step Functions** to orchestrate the reporting creating process that has failover and retry scenarios built into the solution;
- Underlying data to create these reports is stored on **S3**;
- Runs into hundreds of Terabytes and should be available with **millisecond latency**;
- What is the **MOST cost-effective storage class?**
- Answer:
	- Can't be **S3 Standard** even if the data is available in millisecond latency, it isn't the cheapest option;
	- Can't be **Glacier Deep Archive** since objects stored there can take **ATLEAST 12 HOURS** and have a min. storage duration of **180 days or 6 months**;
	- Can't be **S3 Intelligent-Tiering** even if it is designed to optimize costs, it isn't cheaper than **S3 Standard IA** and offers the same availability, durability and throughput, making it not-worth.
	- <mark class="hltr-green">Since the data is accessed only TWICE in a financial year but needs rapid access when required, Standard IA is the most cost-effective storage class for this.</mark>
	- <mark class="hltr-green">S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed.</mark>
	- <mark class="hltr-green">S3 Standard IA matches the high durability, high throughput and low latency of S3 Standard with a <mark class="hltr-red">low per GB storage price and per GB retrieval fee.</mark></mark>
	- <mark class="hltr-green">S3 Standard IA is designed for 99.9% availability compared to 99.99% of S3 Standard. However the report creation has failover and retry scenarios built into the workflow.</mark>


## Scenario 10
- News Network Company uses S3 to aggregate the raw video footage from reporting teams across the US;
- They recently expanded into new geographies in **Europe and Asia**;
- Tech teams at overseas branch offices have reported **huge delays in uploading large video files to the destination S3 bucket**;
- Which is the **MOST cost-effective** option to improve the file upload speed into S3?
- Answer:
	- Can't be **Create multiple AWS direct connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into S3**. Direct Connect (DX) is not cheap and can take months to be provisioned.
	- Can't be **Create multiple site-to-site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into S3** because Site-to-Site VPN won't help accelerating file transfer speed;
	- Can't be **Use AWS Global Accelerator for faster file uploads into the destination S3 bucket** will not help accelerating S3 file transfer speed;
	- <mark class="hltr-green">Use Amazon S3 Transfer Acceleration to enable faster file uploads into the destination S3 bucket is the right answer. It uses CloudFront edge locations to make transfer faster as the data arrives at an edge location and is routed to S3 over an optimized network path.</mark> 

## Scenario 11
- What lifecycle transition is invalid for storage classes?
- Answer:
	- <mark class="hltr-green">S3 Intelligent Tiering -> S3 Standard</mark>
	- <mark class="hltr-green">S3 One Zone-IA -> S3 Standard-IA</mark>

## Scenario 12
- Company measures what consumers watch and what advertising they're exposed to;
- This **real-time data** is ingested into its **on-premises** data center and subsequently, the daily data feed is compressed into a single file and uploaded on **S3** for backup;
- Typical compressed file size is around 2GB;
- Which is the fastest way to upload the daily compressed file into s3?
- Answer:
	- <mark class="hltr-green">Multipart upload with S3 transfer acceleration;</mark>


## Scenario 13
#s3 #ebs 
- Company wants to compare pricing for storage types on AWS;
- DevOps creates a test file of size 1GB with some random data;
- Next he copies this test file **into S3 Standard** Storage Class;
- Next he provisions an EBS Volume (General Purpose SSD (gp2)) with **100GB of provisioned storage** and copies the test file into the EBS volume;
- Lastly copies the test file into an **EFS Standard Storage filesystem**;
- What is the correct order of charges incurred for the file on these three storage types?
- Answer:
	- S3 Standard (**$0.023**) - EFS (**$0.30**) - EBS(gp2)(**$10**)