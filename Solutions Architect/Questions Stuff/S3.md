#s3 #s3-bucket-url
- Valid URLs for bucket:
		- https://bucketteste.s3.us-east-1.amazonaws.com
			- https:// **(bucket)** . s3 . **(region)** . amazonaws.com
		- https://s3.us-east-1.amazonaws.com/bucketteste
			- https:// s3 . **(region)** . amazonaws.com/ **(bucket)**
##### Scenario 01
#s3 #s3-charges #s3ta
- NASA scientist is trying to upload high-res image to S3;
- Image size is 3GB;
- He is using S3 Transfer Acceleration;
- S3TA did not result in an accelerated transfer;
- How AWS would charge this image transfer?
- Answer:
	- Cannot be any option that says "**AWS charges S3 transfer for the image upload.**"
	- <mark style="background: #BBFABBA6;">AWS won't charge for the image upload, because with S3TA you pay ONLY for transfers that ARE INDEED accelerated.</mark>

##### Scenario 02
#s3 #s3-security #s3-encryption #s3-sse-kms
- Company is build an application;
- It will deal with **sensitive health information**;
- Backups of the user data **must be kept encrypted in S3**;
- Company does not want to **provide its own encryption keys**;
- **But** still wants to **maintain an audit trail of when an encryption key _was used and by whom_**;
- Which is the best solution?
- Answer:
	- Can't be **client-side encryption** since company doesn't want to provide its own encryption keys;
	- Can't be **SSE-S3** since company wants to maintain an audit trail and with SSE-S3 each object is encrypted with an **_unique key_**;
	- Can't be **SSE-C** since you can't audit trail the usage of the encryption keys, and the company doesn't want to provide its own encryption keys;
	- <mark style="background: #BBFABBA6;">SSE-KMS is the right answer since it takes away the need for the company to provide its own keys AND you can maintain an audit trail of when the key was used and by whom.</mark>

##### Scenario 03
- Company uses S3 to store sensitive customer data;
- Company has defined different retention periods for different objects based on compliance requirements;
- **Retention rules aren't working as expected**;
- Which one is a valid configuration for **setting up retention periods for objects in S3 buckets**;
- Answer:
	- The bucket's default settings will **NOT** override any explicit retention mode or period you request on an object version;
	- You actually **CAN** place a retention period on an object version either explicitly or through a bucket default setting;
	- You **CANNOT** specify a `Retain Until Date` for the object version. Instead you specify a `Duration`.
	- <mark style="background: #BBFABBA6;">Different versions of a single object can have different retention modes and periods;</mark>
	- <mark style="background: #BBFABBA6;">When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version.</mark>

##### Scenario 04
#s3 #s3-limits #s3-scaling
- Company uses S3 as file-hosting service;
- Customer files are uploaded directly to S3 under a **single bucket**;
- Company started seeing scalability issues where customer file uploads have started failing during the peak access hours with more than **5000 requests per second**;
- **MOST resource-efficient** and **cost-optimal** way of addressing this issue?
- Answer:
	- Change the app's architecture to create a new S3 bucket for each day's data and then upload the daily files directly under that day's bucket **is inefficient and s3 buckets must be globally unique**; 
	- Change the app's architecture to create a new S3 bucket for each customer and then upload each customer's file directly under the respective bucket **is inefficient and s3 buckets must be globally unique**; 
	- Change the app's architecture to use EFS instead of S3: **EFS is costlier compared to S3**
	- <mark style="background: #BBFABBA6;">Change the app's architecture to create a Customer-Specific custom prefixes within a SINGLE BUCKET and then upload the daily files into those prefixes locations.</mark>
	- <mark style="background: #BBFABBA6;">S3 automatically scales to high request rates. For example: your application can achieve at least <mark style="background: #FFB86CA6;">3.500 PUT/COPY/POST/DELETE</mark> or <mark style="background: #FFB86CA6;">5.500 GET/HEAD</mark> requests <mark style="background: #FF5582A6;">PER SECOND PER PREFIX WITHIN A SINGLE BUCKET</mark>. </mark>
	- <mark style="background: #BBFABBA6;">For example, if you create<mark style="background: #7ce616;">10 prefixes</mark> in an S3 bucket to parallelize reads, you could scale your read performance to <mark style="background: #ef6c00;">55,000 READ REQUESTS PER SECOND!</mark></mark>

##### Scenario 05
#s3 #s3-lifecycle 
- Company stores its assets on S3;
- Assets are accessed by a large number of users for the first few days;
- Then the frequency of **access falls down drastically after a week**;
- Although the assets would be accessed occasionally after the first week, they **must continue to be immediately accessible when required**;
- Company wants to reduce cost as much as possible.
- Can you suggest a way to lower storage cost while fulfilling the business requirements?
- Answer:
	- Can't be **Lifecycle policy -> S3 Standard IA** after 30 days. It fulfills **almost** all requirements but isn't the **cheapest one.**;
	- Can't be **Lifecycle Policy -> One Zone-IA** after 7 days because the minimum storage duration is **_30 days before you can transition objects from Standard to One Zone-IA or Standard-IA_**
	- Can't be **Lifecycle Policy -> Standard-IA** after 7 days because the minimum storage duration is **_30 days before you can transition objects from Standard to One Zone-IA or Standard-IA_**
	- <mark style="background: #BBFABBA6;">Lifecycle Policy -> One-Zone IA after 30 days is the right answer since it respects the minimum storage duration to transition objects (30 days) and is the cheapest option.</mark>
